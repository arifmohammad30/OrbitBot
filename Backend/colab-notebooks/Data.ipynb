{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1JAtvSuqnMcJiITUhhfjyYyBbD82CmQH8","authorship_tag":"ABX9TyMiAhL4KaiRxtX39vFRyXwF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B9d-5ZsMszc7","executionInfo":{"status":"ok","timestamp":1751884679430,"user_tz":-330,"elapsed":21184,"user":{"displayName":"Lakshman","userId":"05458583365729094299"}},"outputId":"fca2a957-aa2b-4047-8f85-2aa1035eb254"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… All necessary libraries are installed to their latest versions.\n"]}],"source":["# 1. SETUP: Ensure the correct, latest libraries are installed\n","# ------------------------------------------------------------------------------\n","!pip install -q --upgrade \"crawl4ai>=0.6.0\" \"pyOpenSSL\"\n","!pip install -q pandas lxml requests\n","print(\"âœ… All necessary libraries are installed to their latest versions.\")"]},{"cell_type":"code","source":["#Deep Crawling\n","\n","!  playwright install\n","import asyncio\n","import os\n","from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode, BrowserConfig\n","from crawl4ai.deep_crawling import BFSDeepCrawlStrategy\n","from crawl4ai.deep_crawling.filters import FilterChain, URLPatternFilter\n","\n","# --- Configuration ---\n","START_URL = \"https://www.mosdac.gov.in\"\n","URL_LIST_FILE = \"/content/discovered_urls.txt\" # Ensure this path is correct for your Colab environment\n","\n","async def run_true_deep_crawl():\n","    \"\"\"\n","    This script performs a TRUE deep crawl, discovering all links on each page\n","    and exploring them up to the specified depth, ensuring ALL relevant URLs\n","    (including documents) are discovered by relaxing content type filters during discovery.\n","    \"\"\"\n","    print(f\"ðŸš€ Starting TRUE deep crawl on {START_URL}. This may take several minutes...\")\n","\n","    # 1. Define filters for URL patterns ONLY.\n","    # We explicitly REMOVE the ContentTypeFilter from here.\n","    # The goal of this phase is *discovery* of URLs, not content filtering.\n","    filter_chain = FilterChain([\n","        # Filter 1: Only allow URLs that belong to our specified domain.\n","        # This is a positive match: ONLY allow URLs that start with our base URL.\n","        URLPatternFilter(patterns=[f\"{START_URL}/*\"]),\n","    ])\n","\n","    # 2. Configure the deep crawl strategy.\n","    deep_crawl_strategy = BFSDeepCrawlStrategy(\n","        max_depth=3,  # Go two \"clicks\" deep from the starting URL.\n","        include_external=False,  # Explicitly prevents crawling external domains.\n","        filter_chain=filter_chain, # Apply only the URL pattern filter.\n","    )\n","\n","    # 3. Create a CrawlerRunConfig for the crawl execution.\n","    # 'stream=True' ensures results are processed as they become available.\n","    config = CrawlerRunConfig(\n","        deep_crawl_strategy=deep_crawl_strategy,\n","        cache_mode=CacheMode.ENABLED, # Enable caching for efficiency on repeated runs.\n","        stream=True # Stream results to process them incrementally.\n","    )\n","\n","    # 4. Initialize and run the crawler.\n","    # BrowserConfig controls browser-level settings like headless mode.\n","    discovered_urls = set() # Use a set to automatically handle duplicates.\n","    async with AsyncWebCrawler(config=BrowserConfig(headless=True, verbose=False)) as crawler:\n","        async for result in await crawler.arun(START_URL, config=config):\n","            if result.success:\n","                # --- POST-DISCOVERY FILTERING FOR SPECIFIC IRRELEVANT URL PATTERNS ---\n","                # This catches problematic URLs that we don't want to process further,\n","                # like mailto links or specific internal non-content pages, even if they were discovered.\n","                if \"mailto:\" in result.url or \\\n","                   \"/internal/logout\" in result.url or \\\n","                   \"/internal/registration\" in result.url or \\\n","                   \"/internal/uops\" in result.url:\n","                    print(f\"  [DISCOVERED BUT SKIPPING SAVE] {result.url} (filtered during post-discovery)\")\n","                    continue\n","\n","                print(f\"  [OK] Discovered: {result.url}\")\n","                discovered_urls.add(result.url)\n","            else:\n","                # Log pages that couldn't be crawled (e.g., due to network issues or genuinely invalid responses).\n","                # Crucially, direct file URLs should no longer be hitting ContentTypeFilter issues here.\n","                print(f\"  [SKIPPED] Could not crawl {result.url}: {result.error_message}\")\n","\n","    print(\"\\n--- Deep Crawl Complete! ---\")\n","    print(f\"Discovered {len(discovered_urls)} unique pages.\")\n","\n","    # 5. Save the comprehensive list of unique URLs to our file.\n","    with open(URL_LIST_FILE, \"w\") as f:\n","        for url in sorted(list(discovered_urls)):\n","            f.write(url + \"\\n\")\n","\n","    print(f\"Comprehensive URL list saved to '{URL_LIST_FILE}'.\")\n","\n","# Run the deep crawl function for Colab environment.\n","await run_true_deep_crawl()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ZNGXlVs-_pb3lzZ6Bm0lZnEk9pN2dPNl"},"id":"tpkxmtPVuXB-","executionInfo":{"status":"ok","timestamp":1751884782421,"user_tz":-330,"elapsed":102979,"user":{"displayName":"Lakshman","userId":"05458583365729094299"}},"outputId":"09b1811e-debc-4260-e62a-56a62c797ca5"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["import asyncio\n","import json\n","import os\n","import re\n","from bs4 import BeautifulSoup, Comment\n","from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n","from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n","from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n","import base64\n","\n","from typing import List, Dict, Union\n","from urllib.parse import urljoin\n","\n","# --- Configuration ---\n","URL_LIST_FILE = \"/content/discovered_urls.txt\"\n","# Set OUTPUT_DIR to your Google Drive mounted path for persistence\n","OUTPUT_DIR = \"/content/drive/MyDrive/extracted_content\"\n","\n","# Create output directories if they don't exist\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","os.makedirs(os.path.join(OUTPUT_DIR, \"structured_json\"), exist_ok=True)\n","os.makedirs(os.path.join(OUTPUT_DIR, \"unstructured_markdown\"), exist_ok=True)\n","os.makedirs(os.path.join(OUTPUT_DIR, \"pdfs\"), exist_ok=True)\n","os.makedirs(os.path.join(OUTPUT_DIR, \"extracted_tables\"), exist_ok=True)\n","os.makedirs(os.path.join(OUTPUT_DIR, \"extracted_links\"), exist_ok=True)\n","os.makedirs(os.path.join(OUTPUT_DIR, \"debug_html\"), exist_ok=True)\n","\n","# --- Helper functions for BeautifulSoup parsing ---\n","async def parse_faq_html_with_bs(html_content: str) -> List[Dict[str, str]]:\n","    \"\"\"\n","    Parses FAQ HTML content using BeautifulSoup to extract questions and answers.\n","    Includes aggressive cleaning of MSO XML remnants and deduplication of answer text.\n","    \"\"\"\n","    faq_items = []\n","    soup = BeautifulSoup(html_content, 'html.parser')\n","\n","    faq_containers = soup.select('div.faq-question-answer')\n","\n","    for container in faq_containers:\n","        question_element = container.select_one('div.faq-question a')\n","        answer_element = container.select_one('div.faq-answer')\n","\n","        question = question_element.get_text(strip=True) if question_element else \"\"\n","\n","        answer = \"\"\n","        if answer_element:\n","            for element in answer_element(string=lambda text: isinstance(text, Comment)):\n","                element.extract()\n","            for s in answer_element([\"script\", \"style\", \"link\"]):\n","                s.decompose()\n","\n","            unique_answer_parts = set()\n","            for tag in answer_element.find_all(re.compile(r\"^(p|span|div|a|li|h[1-6])$\")):\n","                text_content = tag.get_text(strip=True)\n","\n","                text_content = re.sub(r'<\\/?w:[^>]+>', '', text_content)\n","                text_content = re.sub(r'<\\/?xml:namespace[^>]+>', '', text_content)\n","                text_content = re.sub(r'', '', text_content, flags=re.DOTALL)\n","                text_content = re.sub(r'[\\s\\uFEFF\\xA0]+', ' ', text_content).strip()\n","\n","                if text_content:\n","                    unique_answer_parts.add(text_content)\n","\n","            answer = \"\\n\".join(sorted(list(unique_answer_parts)))\n","\n","\n","        if question and answer:\n","            faq_items.append({\"question\": question, \"answer\": answer})\n","\n","    return faq_items\n","\n","async def parse_table_html_with_bs(html_content: str, table_css_selector: str) -> List[Dict[str, str]]:\n","    \"\"\"\n","    Parses specific table HTML content using BeautifulSoup.\n","    Assumes the table is fully rendered in the provided HTML content.\n","    This is for the 'sticky-enabled' type tables.\n","    \"\"\"\n","    table_rows = []\n","    soup = BeautifulSoup(html_content, 'html.parser')\n","\n","    target_table = soup.select_one(table_css_selector)\n","\n","    if target_table:\n","        for row_element in target_table.select('tbody tr'):\n","            cells = row_element.select('td')\n","            if len(cells) >= 3:\n","                display_name = \"\"\n","                display_link = \"\"\n","                modified_date = \"\"\n","\n","                name_link_element = cells[1].select_one('a')\n","                if name_link_element:\n","                    display_name = name_link_element.get_text(strip=True)\n","                    display_link = name_link_element.get('href')\n","                    if display_link and not display_link.startswith('http'):\n","                        display_link = urljoin(\"https://www.mosdac.gov.in\", display_link)\n","\n","                modified_date_element = cells[2]\n","                if modified_date_element:\n","                    modified_date = modified_date_element.get_text(strip=True)\n","\n","                table_rows.append({\n","                    \"display_name\": display_name,\n","                    \"display_link\": display_link,\n","                    \"modified_date\": modified_date\n","                })\n","    return table_rows\n","\n","async def parse_angular_table_html_with_bs(html_content: str, table_css_selector: str) -> List[Dict[str, str]]:\n","    \"\"\"\n","    Parses AngularJS table HTML content using BeautifulSoup.\n","    Assumes the content is fully rendered by Angular/JS in the provided HTML.\n","    This is for tables with id=\"tabledata\".\n","    \"\"\"\n","    table_rows = []\n","    soup = BeautifulSoup(html_content, 'html.parser')\n","\n","    target_table = soup.select_one(table_css_selector)\n","\n","    if target_table:\n","        for row_element in target_table.select('tbody tr'):\n","            cells = row_element.select('td')\n","\n","            # Based on the HTML provided, these tables have 9 columns for data (excluding Sr.No, including DOI)\n","            if len(cells) >= 9:\n","                row_data = {}\n","\n","                row_data[\"Sr_No\"] = cells[0].get_text(strip=True)\n","\n","                product_name_element = cells[1].select_one('b')\n","                row_data[\"Product_Name\"] = product_name_element.get_text(strip=True) if product_name_element else \"\"\n","                # We are skipping direct extraction of ng-click/ng-href from md-icon as it's complex without LLM or Angular-aware parser.\n","\n","                row_data[\"Product_Description\"] = cells[2].get_text(strip=True)\n","                row_data[\"Processing_Level\"] = cells[3].get_text(strip=True)\n","                row_data[\"Temporal_Resolution\"] = cells[4].get_text(strip=True)\n","                row_data[\"Start_Date\"] = cells[5].get_text(strip=True)\n","                row_data[\"End_Date\"] = cells[6].get_text(strip=True)\n","                row_data[\"Processing_Status\"] = cells[7].get_text(strip=True)\n","\n","                doi_link_element = cells[8].select_one('a')\n","                row_data[\"DOI\"] = doi_link_element.get_text(strip=True) if doi_link_element else \"\"\n","                # Attempt to get the href. If ng-href isn't resolved to href, it will be None or template.\n","                row_data[\"DOI_Link\"] = doi_link_element.get('href') if doi_link_element else \"\"\n","\n","                table_rows.append(row_data)\n","    return table_rows\n","\n","async def extract_links_from_markdown_file(markdown_text: str, source_page_url: str) -> List[Dict[str, str]]:\n","    \"\"\"\n","    Extracts all Markdown-formatted links from a given Markdown text.\n","    Resolves relative URLs to absolute URLs and captures context.\n","    \"\"\"\n","    extracted_links = []\n","\n","    markdown_link_pattern = re.compile(r'\\[(.*?)\\]\\((.*?)\\)')\n","\n","    seen_links = set()\n","\n","    for match in markdown_link_pattern.finditer(markdown_text):\n","        full_match_text = match.group(0)\n","        link_text = match.group(1).strip()\n","        relative_url = match.group(2).strip()\n","\n","        if relative_url:\n","            absolute_url = urljoin(source_page_url, relative_url)\n","            if not (absolute_url.startswith(\"http://\") or absolute_url.startswith(\"https://\")):\n","                continue\n","\n","            start_index = match.start()\n","            end_index = match.end()\n","\n","            context_window = 150\n","\n","            context_before_raw = markdown_text[max(0, start_index - context_window):start_index]\n","            context_after_raw = markdown_text[end_index:min(len(markdown_text), end_index + context_window)]\n","\n","            context_snippet_raw = f\"{context_before_raw}{full_match_text}{context_after_raw}\"\n","            context_snippet = re.sub(r'\\s+', ' ', context_snippet_raw).strip()\n","            if start_index > context_window:\n","                context_snippet = \"...\" + context_snippet\n","            if end_index + context_window < len(markdown_text):\n","                context_snippet = context_snippet + \"...\"\n","\n","            link_entry = {\n","                \"source_page\": source_page_url,\n","                \"link_text\": link_text,\n","                \"target_url\": absolute_url,\n","                \"context_snippet\": context_snippet\n","            }\n","\n","            link_hash = json.dumps(link_entry, sort_keys=True)\n","            if link_hash not in seen_links:\n","                extracted_links.append(link_entry)\n","                seen_links.add(link_hash)\n","\n","    return extracted_links\n","\n","\n","async def process_page_with_custom_parsing(crawler: AsyncWebCrawler, url: str, content_type: str, output_filename: str, js_script: str = None, wait_for_js_condition: str = None):\n","    \"\"\"\n","    Fetches a page, applies JS/wait conditions, and then uses custom BeautifulSoup parsing.\n","    Dispatches to the correct parsing function based on content_type.\n","    \"\"\"\n","    print(f\"  -> Running CUSTOM BS parsing for {content_type} on {url}\")\n","\n","    config = CrawlerRunConfig(\n","        cache_mode=CacheMode.BYPASS,\n","        wait_until=\"networkidle\",\n","        js_code=js_script,\n","        wait_for=wait_for_js_condition,\n","    )\n","\n","    try:\n","        result = await crawler.arun(url, config=config)\n","\n","        if result.success and result.html:\n","            parsed_data = []\n","            if content_type == \"faq\":\n","                parsed_data = await parse_faq_html_with_bs(result.html)\n","            elif content_type == \"table\": # For 'sticky-enabled' tables\n","                table_css_selector = \"table.sticky-enabled.tableheader-processed.sticky-table\"\n","                parsed_data = await parse_table_html_with_bs(result.html, table_css_selector)\n","            elif content_type == \"angular_table\": # For new AngularJS tables\n","                table_css_selector = \"table#tabledata\" # Use specific ID for Angular table\n","                parsed_data = await parse_angular_table_html_with_bs(result.html, table_css_selector)\n","\n","            if parsed_data:\n","                if content_type == \"faq\":\n","                    filepath = os.path.join(OUTPUT_DIR, \"structured_json\", f\"{output_filename}.json\")\n","                elif content_type == \"table\" or content_type == \"angular_table\":\n","                    filepath = os.path.join(OUTPUT_DIR, \"extracted_tables\", f\"{output_filename}.json\")\n","\n","                with open(filepath, \"w\", encoding=\"utf-8\") as f:\n","                    json.dump(parsed_data, f, indent=2)\n","                print(f\"  âœ… Saved custom-parsed {content_type} data to {filepath}\")\n","            else:\n","                print(f\"  âŒ Custom BS parsing yielded empty data for {content_type} on {url}. HTML length: {len(result.html)}. Raw HTML snippet (first 1000 chars): {result.html[:1000]}\")\n","                debug_filepath = os.path.join(OUTPUT_DIR, \"debug_html\", f\"{output_filename}_bs_empty_output.html\")\n","                with open(debug_filepath, \"w\", encoding=\"utf-8\") as f:\n","                    f.write(result.html)\n","                print(f\"    (Debug: Saved problematic HTML for BS parsing to {debug_filepath})\")\n","        else:\n","            print(f\"  âŒ Failed to fetch HTML for custom parsing of {content_type} on {url}. Error: {result.error_message}\")\n","            if result.html:\n","                debug_filepath = os.path.join(OUTPUT_DIR, \"debug_html\", f\"{output_filename}_bs_fetch_failed_html.html\")\n","                with open(debug_filepath, \"w\", encoding=\"utf-8\") as f:\n","                    f.write(result.html)\n","                print(f\"    (Debug: Saved problematic HTML to {debug_filepath})\")\n","    except Exception as e:\n","        print(f\"  âŒ Exception during custom BS parsing for {content_type} on {url}: {e}\")\n","\n","\n","async def extract_unstructured_page(crawler: AsyncWebCrawler, url: str, output_filename: str):\n","    \"\"\"\n","    Extractor for unstructured text and native tables.\n","    Now also extracts links from the markdown content.\n","    \"\"\"\n","    print(f\"  -> Running UNSTRUCTURED extraction on {url}\")\n","    try:\n","        md_generator = DefaultMarkdownGenerator()\n","\n","        config = CrawlerRunConfig(\n","            markdown_generator=md_generator,\n","            cache_mode=CacheMode.ENABLED,\n","            process_iframes=True,\n","            table_score_threshold=0\n","        )\n","        result = await crawler.arun(url, config=config)\n","\n","        if result.success:\n","            markdown_to_save = None\n","            if result.markdown and result.markdown.fit_markdown and len(result.markdown.fit_markdown.strip()) > 50:\n","                markdown_to_save = result.markdown.fit_markdown\n","                print(f\"  â„¹ï¸ Using fit_markdown for {url} (length: {len(markdown_to_save)} chars)\")\n","            elif result.markdown and result.markdown.raw_markdown and len(result.markdown.raw_markdown.strip()) > 50:\n","                markdown_to_save = result.markdown.raw_markdown\n","                print(f\"  âš ï¸ Warning: fit_markdown too short for {url}. Using raw_markdown (length: {len(markdown_to_save)} chars)\")\n","            elif result.html and len(result.html.strip()) > 50:\n","                soup = BeautifulSoup(result.html, 'html.parser')\n","                main_content_tag = soup.find('main') or soup.find('article') or soup.find('div', id='content')\n","                markdown_to_save = main_content_tag.get_text(separator='\\n', strip=True) if main_content_tag else soup.get_text(separator='\\n', strip=True)\n","                print(f\"  âš ï¸ Warning: No markdown generated for {url}. Falling back to BeautifulSoup text extraction (length: {len(markdown_to_save)} chars)\")\n","\n","            if markdown_to_save and len(markdown_to_save.strip()) > 50:\n","                filepath = os.path.join(OUTPUT_DIR, \"unstructured_markdown\", f\"{output_filename}.md\")\n","                with open(filepath, \"w\", encoding=\"utf-8\") as f:\n","                    f.write(markdown_to_save)\n","                print(f\"  âœ… Saved markdown content to {filepath}\")\n","\n","                # --- NEW: Extract links from markdown content with context ---\n","                extracted_links = await extract_links_from_markdown_file(markdown_to_save, url)\n","                if extracted_links:\n","                    links_filepath = os.path.join(OUTPUT_DIR, \"extracted_links\", f\"{output_filename}_links.json\")\n","                    with open(links_filepath, \"w\", encoding=\"utf-8\") as f:\n","                        json.dump(extracted_links, f, indent=2)\n","                    print(f\"  ðŸ”— Extracted {len(extracted_links)} links with context from markdown to {links_filepath}\")\n","                else:\n","                    print(f\"  âž¡ï¸ No links found in markdown for {url}.\")\n","\n","            else:\n","                print(f\"  âŒ Failed to extract any meaningful text content from {url} after all attempts.\")\n","                if result.html:\n","                    debug_filepath = os.path.join(OUTPUT_DIR, \"debug_html\", f\"{output_filename}_no_markdown.html\")\n","                    with open(debug_filepath, \"w\", encoding=\"utf-8\") as f:\n","                        f.write(result.html)\n","                    print(f\"    (Debug: Saved problematic HTML to {debug_filepath})\")\n","\n","            if result.media and \"tables\" in result.media:\n","                if result.media[\"tables\"]:\n","                    print(f\"  ðŸ“Š Native table detection found {len(result.media['tables'])} tables on {url}.\")\n","                else:\n","                    print(f\"  âž¡ï¸ Native table detection found no tables with threshold (0) on {url}.\")\n","            else:\n","                print(f\"  âž¡ï¸ No 'tables' key found in result.media for {url}.\")\n","\n","        else:\n","            print(f\"  âŒ Failed to fetch page content for {url}. Error: {result.error_message}\")\n","    except Exception as e:\n","        print(f\"  âŒ Exception during unstructured extraction on {url}: {e}\")\n","\n","async def download_document(crawler: AsyncWebCrawler, url: str, output_filename: str):\n","    \"\"\"\n","    Handles downloading a document using Crawl4AI's native PDF capture.\n","    \"\"\"\n","    print(f\"  -> Running DOCUMENT download for {url} using native PDF capture.\")\n","\n","    filename = os.path.basename(url.split('?')[0]).replace('%20', ' ')\n","    if not filename:\n","        filename = f\"{output_filename}_download\"\n","\n","    if not any(filename.lower().endswith(ext) for ext in [\".pdf\", \".docx\", \".xlsx\", \".doc\", \".xls\"]):\n","        filename += \".pdf\"\n","\n","    pdf_target_path = os.path.join(OUTPUT_DIR, \"pdfs\", filename)\n","    os.makedirs(os.path.join(OUTPUT_DIR, \"pdfs\"), exist_ok=True)\n","\n","    try:\n","        config = CrawlerRunConfig(\n","            pdf=True,\n","            cache_mode=CacheMode.BYPASS,\n","            page_timeout=60000\n","        )\n","        result = await crawler.arun(url, config=config)\n","\n","        if result.success and result.pdf:\n","            decoded_pdf_content = base64.b64decode(result.pdf)\n","            with open(pdf_target_path, \"wb\", encoding='utf-8') as f: # Specify encoding for text output\n","                f.write(decoded_pdf_content)\n","            print(f\"  âœ… Document downloaded successfully: {pdf_target_path} (size: {len(decoded_pdf_content)} bytes) from {url}\")\n","        else:\n","            print(f\"  âŒ Failed to download document from {url}. Result success: {result.success}. Result.pdf present: {bool(result.pdf)}. Error: {result.error_message}\")\n","            if result.html:\n","                print(f\"  âš ï¸ Note: HTML content was retrieved for {url}. This might not be a direct document link and requires further investigation for content extraction.\")\n","                debug_filepath = os.path.join(OUTPUT_DIR, \"debug_html\", f\"{output_filename}_pdf_download_failed_html.html\")\n","                with open(debug_filepath, \"w\", encoding=\"utf-8\") as f:\n","                    f.write(result.html)\n","                print(f\"    (Debug: Saved problematic HTML for PDF URL to {debug_filepath})\")\n","\n","    except Exception as e:\n","        print(f\"  âŒ Exception during document download on {url}: {e}\")\n","\n","\n","async def main_orchestrator():\n","    \"\"\"\n","    Main orchestrator to read the URL list and delegate to the correct extractor.\n","    \"\"\"\n","    print(\"ðŸš€ Starting Main Extraction Orchestrator (with Markdown Link Extraction!)...\")\n","\n","    if not os.path.exists(URL_LIST_FILE):\n","        print(f\"âŒ Error: '{URL_LIST_FILE}' not found.\")\n","        print(\"Please ensure you've run the 'run_true_deep_crawl' script first to populate this file.\")\n","        return\n","\n","    with open(URL_LIST_FILE, \"r\") as f:\n","        urls = [line.strip() for line in f.readlines()]\n","\n","    browser_config = BrowserConfig(headless=True, accept_downloads=True)\n","\n","    async with AsyncWebCrawler(config=browser_config) as crawler:\n","        for url in urls:\n","            print(f\"\\nProcessing URL: {url}\")\n","\n","            safe_filename = url.replace('https://', '').replace('http://', '').replace('/', '_').replace('?', '_').replace('&', '_').replace('=', '_').replace('.', '_').replace('__', '_')\n","            if not safe_filename:\n","                safe_filename = \"index\"\n","\n","            # --- ROUTING LOGIC ---\n","\n","            # Explicitly skip known problematic/irrelevant URLs from content extraction.\n","            if \"mailto:\" in url or \\\n","               \"/internal/logout\" in url or \\\n","               \"/internal/registration\" in url or \\\n","               \"/internal/uops\" in url:\n","                print(f\"  -> Skipping explicitly excluded URL for content extraction: {url}\")\n","                continue\n","\n","            # Prioritize direct document downloads for file extensions and known download paths\n","            elif url.lower().endswith((\".pdf\", \".docx\", \".xlsx\", \".doc\", \".xls\")) or \\\n","                 \"/docs/\" in url.lower() or \"/look/docs/\" in url.lower() or \\\n","                 \"/filebrowser/download/\" in url.lower().replace('%20', ' '):\n","                print(f\"  -> Detected document for download: {url}\")\n","                await download_document(crawler, url, f\"doc_{safe_filename}\")\n","\n","            # --- FAQ Extraction (Using Custom BeautifulSoup Parsing with aggressive cleaning) ---\n","            elif \"/faq-page\" in url:\n","                print(f\"  -> Detected FAQ page. Running CUSTOM BeautifulSoup extraction for: {url}\")\n","                js_script_faq_expand = \"\"\"\n","                    (async () => {\n","                        const questionElements = document.querySelectorAll('div.faq-question.faq-dt-hide-answer');\n","                        for (let i = 0; i < questionElements.length; i++) {\n","                            const el = questionElements[i];\n","                            if (el.offsetParent !== null) {\n","                                el.click();\n","                                await new Promise(resolve => setTimeout(resolve, 750));\n","                            }\n","                        }\n","                    })();\n","                \"\"\"\n","                wait_for_faq_content = \"js:() => document.querySelectorAll('div.faq-answer:not(.collapsed)').length > 0\"\n","\n","                await process_page_with_custom_parsing(crawler, url, \"faq\", f\"faq_{safe_filename}\", js_script=js_script_faq_expand, wait_for_js_condition=wait_for_faq_content)\n","\n","            # --- Targeted Table Extraction (Using Custom BeautifulSoup Parsing for sticky-enabled) ---\n","            elif \"/data-quality\" in url or \\\n","                 \"/calibration-reports\" in url or \\\n","                 \"/validation-reports\" in url or \\\n","                 \"/insitu\" in url:\n","                print(f\"  -> Detected 'sticky-enabled' table page. Running CUSTOM BeautifulSoup extraction for: {url}\")\n","                await process_page_with_custom_parsing(crawler, url, \"table\", f\"table_{safe_filename}\", js_script=None, wait_for_js_condition=None)\n","\n","            # --- NEW: Targeted AngularJS Table Extraction (Using Custom BeautifulSoup Parsing) ---\n","            # Assuming these URLs contain the AngularJS table (based on product catalog theme)\n","            elif \"internal/catalog-satellite\" in url or \\\n","                 \"internal/catalog-insitu\" in url or \\\n","                 \"internal/catalog-radar\" in url or \\\n","                 \"internal/catalog-insat3a\" in url or \\\n","                 \"internal/catalog-insat3d\" in url or \\\n","                 \"internal/catalog-insat3dr\" in url or \\\n","                 \"internal/catalog-insat3s\" in url or \\\n","                 \"internal/catalog-kalpana1\" in url or \\\n","                 \"internal/catalog-meghatropiques\" in url or \\\n","                 \"internal/catalog-oceansat2\" in url or \\\n","                 \"internal/catalog-oceansat3\" in url or \\\n","                 \"internal/catalog-saral\" in url or \\\n","                 \"internal/catalog-scatsat\" in url:\n","                print(f\"  -> Detected AngularJS table page. Running CUSTOM BeautifulSoup extraction for: {url}\")\n","\n","                # JavaScript wait for the first dynamically rendered cell to have content\n","                js_wait_for_angular_content = \"js:() => document.querySelector('table#tabledata tbody tr:first-child td:nth-child(2) b').textContent.trim().length > 0\"\n","\n","                await process_page_with_custom_parsing(\n","                    crawler,\n","                    url,\n","                    \"angular_table\", # Content type identifier for dispatching\n","                    f\"angular_table_{safe_filename}\",\n","                    js_script=None, # No specific Angular interaction JS needed, relying on load state and wait_for\n","                    wait_for_js_condition=js_wait_for_angular_content\n","                )\n","\n","            # Generic skipping of common navigational or less content-rich pages\n","            elif any(keyword in url for keyword in [\"/about-us\", \"/contact-us\", \"policy\", \"/sitemap\", \"/gallery\", \"/rss\", \"announcements\"]):\n","                print(f\"  -> Skipping generic navigational/policy page: {url}\")\n","                continue\n","\n","            # Handle XML files as unstructured text\n","            elif url.lower().endswith(\".xml\"):\n","                print(f\"  -> Treating XML file as UNSTRUCTURED text. URL: {url}\")\n","                await extract_unstructured_page(crawler, url, f\"xml_page_{safe_filename}\")\n","\n","            # Dynamic tabbed content pages (e.g., /node?qt-latest_products=1)\n","            elif \"node?qt\" in url:\n","                print(f\"  -> Treating dynamic tab page as UNSTRUCTURED for now. URL: {url}\")\n","                await extract_unstructured_page(crawler, url, f\"dynamic_tab_{safe_filename}\")\n","\n","            # Internal section pages (e.g., /internal/aws, /internal/cyclone)\n","            elif \"/internal/\" in url:\n","                print(f\"  -> Treating /internal/ page as UNSTRUCTURED for now. URL: {url}\")\n","                await extract_unstructured_page(crawler, url, f\"internal_page_{safe_filename}\")\n","\n","            else:\n","                # Default to unstructured extraction for all other HTML pages not specifically routed.\n","                await extract_unstructured_page(crawler, url, f\"page_{safe_filename}\")\n","\n","    print(\"\\n--- Orchestrator Finished! ---\")\n","    print(f\"Check the '{OUTPUT_DIR}' directory for extracted content.\")\n","\n","await main_orchestrator()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"16jqUHBgFAc_9jerztBsp5iDFnXm_kTSa"},"id":"-FIR7x3Twas7","executionInfo":{"status":"ok","timestamp":1751886274018,"user_tz":-330,"elapsed":457248,"user":{"displayName":"Lakshman","userId":"05458583365729094299"}},"outputId":"bde34674-e68f-4fd3-de58-d83d7cbf7bb6"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"cKj7RxO9MtCJ","executionInfo":{"status":"aborted","timestamp":1751884772533,"user_tz":-330,"elapsed":114495,"user":{"displayName":"Lakshman","userId":"05458583365729094299"}}},"execution_count":null,"outputs":[]}]}